# 查询截取分析

- 分析
    - 观察，至少跑一天，看看生产的慢SQL情况
    - 开启慢查询日志，设置阈值，比如超过5秒钟的就是慢SQL，并将它抓取出来。
    - `explain` + `慢SQL` 分析
    - `show profile`
    - 数据库参数调优

- 总结
    - 慢查询的开启并捕获
    - `explain` + `慢SQL` 分析
    - `show profile` 查询 `SQL` 在 `Mysql` 服务器里面的执行细节和生命周期情况
    - SQL数据库的参数调优
    
## 查询优化

### 永远小表驱动大表

优化原则：小表驱动大表，即小的数据集驱动大的数据集。

1. 什么时候用 `in`

    ```
    select * from A where id in (select id from B);
    等价于
    for select id from B
        for select * from A where A.id=B.id 
    ```
   
    **当B表的数据集必须小于A表的数据集的时候,用 `in` 优于用 `exists`**

2. 什么时候用 `exists`
    ```
    select * from A where exists (select 1 from B where B.id = A.id);
    
    等价于
    
    for select * from A
        for select * from B where B.id = A.id;
    ```
    **当A表的数据集小于B表的数据集的时候，用`exists`优于用`in`**
    
    **A表与B表的ID字段应建立索引**
3. `EXISTS` 子句的理解
- `select ... from table where exists(subquery)`
- 改语法可以理解为
    - **将主查询的数据，放到子查询中做条件验证**
    - **根据验证结果（TRUE 或 FALSE）来决定主查询的数据结果是否得以保留** 
4. 提示
- `EXISTS(subquery)`只返回 TRUE 或 FALSE
    - 因此子查询中的 `select * ` 也可以是 `select1` 或者 `select 'X'`
    - 官方的说法是实际执行的时候会忽略 `SELECT` 清单，因此没有区别
- `EXISTS`子查询的实际执行过程可能经过了优化
    - 而不是我们理解的逐条对比
    - 如果担心效率问题，可以进行实际检验以确定是否有效率问题
- `EXISTS`子查询往往也可以用条件表达式，其他子查询或者JOIN替代，何种最优需要具体问题具体分析

### `order by` 关键字优化

1. `order by` 字句，尽量使用 `Index`方式排序，避免使用`FileSort`方式排序
    - **重点在于会不会产生 `filesort`**
2. 尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀
3. 如果不在索引列上，filesort 有两种算法，mysql就要启动双路排序和单路排序
    - 双路排序讲解
        - Mysq4.1之前是使用双路排序，字面的意思就是**两次**扫描磁盘，最终得到数据
        > 读取行指针和orderby的列，对他们进行排序，然后扫描已经排好序的列表，按照列表中的值重新从列表中读取对应的数据输出
    - 从磁盘读取一批数据，要对磁盘进行两次扫描，众所周知，I/O是很耗时的，所以在Mysql4.1之后，使用单路排序
    - 单路排序讲解
    > 从磁盘读取查询所需要的列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了二次数据读取。而且把随机IO变成了顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了
    - 引申出的问题
        - 优于单路是后厨的，总体而言好过双路
        - 但是单路有问题
        > 在sort_buffer中，方法B比方法A要多占用很多空间，因为方法B是把所有字段都取出，所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排...从而导致多次IO
        > <br>本来想省一次IO操作，反而导致了大量的IO操作，反而得不偿失
4. 优化策略
    - 增大sort_buffer_size参数的设置
    - 增大max_length_for_sort_data参数的设置
    - Why 提高OrderBy的速度
        - order by 时 `select *` 是一个大忌，只query需要的字段，这点非常重要
            - 当query的字段大小的综合小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB类型时，会用改进后的算法--单路排序，否则用老算法--多路排序
            - 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次IO，但是用单路排序算法的风险会更大一些，所以要提高sort_buffer_size
        - 尝试提高 `sort_buffer_size`
        - 尝试提高 `max_length_for_sort_data`
5. 小总结
    - order by 语句使用索引最左前列
    - 使用 Where 字句与 Order By 字句条件列组合满足最左前列
    ```
    KEY a_b_c(a,b,c)
   
    order by 能使用最左前缀
    1.order by a
    2.order by a,b
    3.order by a,b,c
    4.order by a desc,b desc,c desc
    ```
    - **如果 where 使用索引的最左前缀定义为常量，则order by能使用索引**
    ```
    where a=const order by b,c
    where a=const and b=const order by c
    where a=const and b>const order by b,c
    ```
    - **不能使用索引排序的情况**
    ```
    order by a asc,b desc,c desc // 排序不一致
    where g=const order by b,c // 丢失a索引，带头大哥不在
    where a=const order by c // 丢失b的索引，中间兄弟断了
    where a=const order by a,d // d不是索引的一部分
    where a in (...) order by b,c // 对于排序俩说，多个相等条件也是范围查询
    ```

### `group by` 关键字优化
1. `group by`实质是先排序后进行分组，遵照索引建的最佳左前缀原则
2. 当无法使用索引列，增大 `max_length_for_sort_data` 参数，增大 `sort_buffer_size` 参数
3. `where` 高于 `having`, 能写在 `where` 限定的条件就不要去 `having` 限定了
## 慢查询日志
- `show variables like '%slow_query_log%'` 
- `show variables like '%long_query_time%'` 
    - 时间是大于
- 日志配置
- 查看 `msyqldumpslow` 的帮助信息
    - `s`: 表示按照何种方式排序
    - `c`: 访问次数
    - `l`: 锁定时间
    - `r`: 返回记录
    - `t`: 查询时间
    - `al`: 平均锁定时间
    - `ar`: 平均返回记录数
    - `at`: 平均查询时间
    - `t`: 即为返回前面多少条的数据
    - `g`: 后边搭配一个正则表达式，大小写不敏感
- 工作常用参考
    - 得到返回记录最多的10个sql
        - `msyqldumpslow -s r -t 10 /var/lib/mysql/slow.log` 
    - 得到访问次数最多的10个sql
        - `msyqldumpslow -s c -t 10 /var/lib/mysql/slow.log`
    - 得到按照时间排序的前10条里面含有左连接的查询语句
        - `msyqldumpslow -s t -t -g "left join" 10 /var/lib/mysql/slow`
    - 建议这些命令结合 `more` 使用 否则可能出现爆屏              
        - `msyqldumpslow -s r -t 10 /var/lib/mysql/slow.log | more`                                                                                                                                                                            
## 批量数据脚本
**建表**

```
drop table if exists dept;

create table dept(
    id int unsigned primary key auto_increment,
    deptno mediumint unsigned not null default 0,
    dname varchar(20) not null default '',
    loc varchar(13) not null default ''
) engine = INNODB;


drop table if exists emp;

create table emp(
    id int unsigned primary key auto_increment,
    empno mediumint unsigned not null default 0 comment '编号',
    ename varchar(20) not null default '' comment '名字',
    job varchar(9) not null default '' comment '工作',
    mgr mediumint unsigned not null default 0 comment '上级编号',
    hiredate date not null comment '入职时间',
    sal decimal(7,2) not null comment '薪水',
    comm decimal(7,2) not null comment '红利',
    deptno mediumint unsigned not null default 0 comment '部门编号'
) engine = INNODB;
```

**修改`log_bin_trust_function_creators`参数**
- `show variables like '%log_bin_trust_function_creators%'`
    - 如果是off执行下面的语句
- `set global log_bin_trust_function_creators = 1;`

**创建函数，保证每条数据都不同**
- 随机产生字符串
    ```
    DELIMITER $$
    create function rand_string(n int)
    returns varchar(255)
    BEGIN
        declare chars_str varchar(100) default 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ';
        declare return_str varchar(255) default '';
        declare i int default 0;
        while i < n do
        set return_str = concat(return_str,substring(chars_str,floor(1+rand*52),1));
        set i=i+1;
        end while;
        return return_str;
    END $$
    ```
- 随机产生部门编号
    ```
    DELIMITER $$
    create function rand_num()
    returns int(5)
    begin
        declare i int default 0;
        set i = floor(100+rand()*10);
        return i;
    END $$
    ```
- 假如要删除
    - `drop function rand_num()`

**创建存储过程**
- 创建往emp表中插入数据的存储过程
```
DELIMITER $$
create procedure insert_emp(IN start int(10),IN max_num int(10))
begin
    declare i int default 0;
    set autocommit = 0;
  repeat
    set i = i+1;
        insert into emp(empno,ename,job,mgr,hiredate,sal,comm,deptno) values ((start+i),rand_string(6),'SALESMAN',0001,CURRENT_DATE(),2000,400,rand_num());
    until i = max_num
    end repeat;
    commit;
end $$
```
- 创建往dept表中插入数据的存储过程
```
DELIMITER $$
create procedure insert_dept(IN start int(10),IN max_num int(10))
begin
    declare i int default 0;
    set autocommit = 0;
  repeat
    set i = i+1;
        insert into dept(deptno,dname,loc) values ((start+i),rand_string(10),rand_string(8));
    until i = max_num
    end repeat;
    commit;
end $$
```

**恢复成之前的分隔符**
- `DELIMITER;`

**调用存储过程**
- 创建部门
```
call insert_dept(100,10);
select * from dept;
```
- 创建员工
```
call insert_emp(100001,500000);
```
## Show Profile
**是什么**
- 是mysql提供的可以用来分析当前会话中语句执行的资源消耗情况。可以用于SQL的调优测量
- 默认情况下，参数处于关闭状态，并保存最近15次的运行结果

**分析步骤**
1. 查看是否支持
- `show variables like 'profiling' `
- 如果没有开启
    - `set profiling = on`
        
2. 运行sql 单纯是为了运行一个长时间的sql
    ```
    select * from emp group by id%10 limit 150000;
    
    select * from emp group by id%20 limit 150000;
    ```
3. `show profiles` 一下
- 显示了最近的查询结果
- 关注一下查询后的 `id`
4. 诊断sql
    - 执行 `show profile cpu,block io for query [ID]`
        - 上面关注的id
    - 可以得到某个查询的完整的生命周期和过程
    - `show profile` 还可以带的参数
        - `ALL` : 显示所有的开销
        - `BLOCK IO` : 显示块IO相关开销
        - `CONTEXT SWITCHES` : 上下文切换相关开销
        - `CPU`: 显示CPU相关的开销
        - `IPC`: 显示发送和接收相关开销信息
        - `MEMORY`: 显示内存相关开销信息
        - `PAGE FAULTS`: 显示页面错误相关开销信息
        - `SOURCE`: 显示和 Source_function,Source_file,Source_line相关的开销信息
        - `SWAPS`: 显示交换次数相关开销信息
5. 诊断sql 出现需要关注的东西, 日常开发需要注意的结论
- `convering HEAP to MyISAM`
    - 查询结果太大，内存都不够用了，往磁盘上搬
- `Creating tmp table` 
    - 创建临时表，会导致的问题如下
        - 拷贝数据到临时表
        - 用完再删除
- `copying to tmp table on disk`
    - 把内存中的临时表复制到磁盘，危险！！！
- `locked`

## 全局查询日志
- 日志配置
    - 略
